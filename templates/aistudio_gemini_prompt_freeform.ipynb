{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOpqw5rJbDD5",
        "outputId": "1eb71946-4739-43f7-a7a5-3c6a633aa32d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.jpg  \u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtempfiles\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAMLR5L3MB5yMPaxr7n6c9iP1xYxUtTwGw\")\n",
        "\n",
        "def upload_to_gemini(path, mime_type=None):\n",
        "  \"\"\"Uploads the given file to Gemini.\n",
        "\n",
        "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
        "  \"\"\"\n",
        "  file = genai.upload_file(path, mime_type=mime_type)\n",
        "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "  return file\n",
        "\n",
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 40,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-pro\",\n",
        "  generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# TODO Make these files available on the local file system\n",
        "# You may need to update the file paths\n",
        "files = [\n",
        "  upload_to_gemini(\"2.jpg\", mime_type=\"image/jpeg\"),\n",
        "  upload_to_gemini(\"1.jpg\", mime_type=\"image/jpeg\"),\n",
        "]\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[{\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "        \"该图是一个需要进行区域供热系统规划设计的对象，现有热源和六个负荷，需要你进行负责设计一个供热系统拓扑结构。所构建的拓扑结构需要严格遵循以下：你需要对各个对象和管道交接处设置节点，节点连接代表管道，这些管道不能穿过建筑只在图中的道路进行设计。\\n如果同向有多个负荷，那么该主管道需要进行分支，并且分支后的新节点连接至这两个负荷。\\n同一个节点不能连接超过3个管道，如果有需要设计管道分支。\\n\\n参考：热源节点编号为1；总部基地节点编号为2；商务中心节点编号为3；绿创大厦节点编号为4；北郡节点编号为5；光明府节点编号为6；壹号天禧节点编号为7。你还需要对其他节点额外定义编号。\\n\\n请你用返回一个列表，其中列表元素包含（node_id1,node_id2）代表从节点编号node_id1流入节点编号node_id2。\\n结果模版例如：“[\\n(1, 5), # 热源 -> 节点5\\n(5, 6), # 节点5 -> 节点6\\n(6, 2), # 节点6 -> 总部基地\\n(6, 4), # 节点6 -> 壹号天禧\\n(5, 3) # 节点5 -> 商务中心\\n]”\",\n",
        "      ],\n",
        "    },\n",
        "  {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\n",
        "        \"```\\n[\\n(1, 8),  # 热源 -> 节点8\\n(8, 9),  # 节点8 -> 节点9\\n(9, 2),  # 节点9 -> 总部基地\\n(9, 7),  # 节点9 -> 壹号天禧\\n(8, 10), # 节点8 -> 节点10\\n(10, 11),# 节点10 -> 节点11\\n(11, 5), # 节点11 -> 北郡\\n(11, 6), # 节点11 -> 光明府\\n(10, 3)  # 节点10 -> 商务中心\\n]\\n```\",\n",
        "      ],\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "j04BDYw0a1hF",
        "outputId": "af442cdf-2b47-4a71-d698-026546fb0f55"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file '2.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/ljphcoy0o6zo\n",
            "Uploaded file '1.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/kf183hy50d3r\n",
            "Please provide the input you would like me to process. I need the context or information you want me to work with to give you a relevant response.  For example, you could provide:\n",
            "\n",
            "* **A question:**  \"What is the capital of France?\"\n",
            "* **Text to analyze:** \"This is some text I want you to summarize.\"\n",
            "* **Code to review:**  `print(\"Hello, world!\")`\n",
            "* **A task description:** \"Write a short story about a robot.\"\n",
            "* **An image to describe:** (upload or link to an image)\n",
            "\n",
            "Once you give me input, I can help!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"该图是一个需要进行区域供热系统规划设计的对象，现有热源和六个负荷，需要你进行负责设计一个供热系统拓扑结构。该图有两条主管道，一条是经过绿创大厦、商务中心、总部基地、热源和壹号天禧的纵向，另一条是经过热源、光明府和北郡的横向。\n",
        "所构建的拓扑结构需要严格遵循以下规则：\n",
        "1.你需要将热源和各个负荷通过节点进行连接。节点连接代表管道，但是，这些管道不能穿过建筑只在图中的道路进行设计，你需要不断反思设计的结构是否会穿过图中的建筑或者湖。\n",
        "2.你必须先理解图中各个负荷之间以及热源的相对位置关系。如果该节点同向有两个负荷，那么需要新增支路分支节点，再分别连接至这两个负荷。\n",
        "3.每个新增的支路节点（除了负荷节点和热源节点）必须连接其他3个节点。如果有需要设计管道分支，同方向的管道必须使用相同的主管道。\n",
        "4.热源节点只能作为源节点，负荷节点只能作为目标节点。\n",
        "5.注意！负荷节点之间不能相连。\n",
        "参考：热源节点编号为1；总部基地节点编号为2；商务中心节点编号为3；绿创大厦节点编号为4；北郡节点编号为5；光明府节点编号为6；壹号天禧节点编号为7。你还需要对其他新增的支路节点编号额外进行定义。\n",
        "\n",
        "请你用返回一个列表，其中列表元素包含（源节点node_id1,目标节点node_id2），代表从源节点编号node_id1流入目标节点编号node_id2。\n",
        "结果模版例如：“\n",
        "[\n",
        "(1, 5), # 热源 -> 节点5\n",
        "(5, 6), # 节点5 -> 节点6\n",
        "(6, 2), # 节点6 -> 总部基地\n",
        "(6, 4), # 节点6 -> 壹号天禧\n",
        "(5, 3) # 节点5 -> 商务中心\n",
        "]”\n",
        "(5, 6)主管道\"\"\"\n",
        "\n",
        "history_ = [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[1],\n",
        "        prompt],\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [\"\"\"[\n",
        "(1, 4), # 热源 -> 节点4\n",
        "(4, 2), # 节点4 -> load1\n",
        "(4, 3) # 节点4 -> load2\n",
        "]\"\"\"],\n",
        "    }\n",
        "  ]\n",
        "prompt_input = {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "        prompt],\n",
        "    }\n",
        "for i in range(5):\n",
        "  chat_session = model.start_chat(\n",
        "  history=history_\n",
        "  )\n",
        "  if i ==0:\n",
        "    input = prompt_input.get(\"parts\")[1]\n",
        "  else:\n",
        "    input = prompt_input.get(\"parts\")[0]\n",
        "  response = chat_session.send_message(input)\n",
        "  history_.append(prompt_input)\n",
        "  print(i)\n",
        "  print(response.text)\n",
        "  model_results = {\n",
        "      \"role\": \"model\",\n",
        "      \"parts\": [response.text],\n",
        "    }\n",
        "  history_.append(model_results)\n",
        "\n",
        "  prompt_input = {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        \"请你思考这个结果是否合理，是否遵循规则，如果遵守则输出mark=1，如果不遵守根据规则改进\"\n",
        "        ],\n",
        "    }\n",
        "  history_.append(model_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fBMrPUfjdfoL",
        "outputId": "78795438-d693-4563-9d58-ebdf86d0e75d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "```\n",
            "[\n",
            "(1, 8),  # 热源 -> 节点8\n",
            "(8, 5),  # 节点8 -> 北郡\n",
            "(8, 9),  # 节点8 -> 节点9\n",
            "(5, 6),  # 北郡 -> 光明府\n",
            "(9, 7),  # 节点9 -> 壹号天禧\n",
            "(9, 10), # 节点9 -> 节点10\n",
            "(10, 2), # 节点10 -> 总部基地\n",
            "(10, 3), # 节点10 -> 商务中心\n",
            "(10, 4)  # 节点10 -> 绿创大厦\n",
            "]\n",
            "```\n",
            "\n",
            "**解释和推理过程:**\n",
            "\n",
            "1. **主干道策略:**  题目提示有两条主管道，分析地图和负荷分布，确定两条主管道分别为：\n",
            "    * **主管道1:** 热源 ->  北郡 -> 光明府\n",
            "    * **主管道2:** 热源 -> 壹号天禧 -> 总部基地 -> 商务中心 -> 绿创大厦\n",
            "\n",
            "2. **节点8:** 由于热源需要连接两条主管道，因此在热源附近新增节点8，作为分支点。\n",
            "\n",
            "3. **节点9:** 壹号天禧之后，需要连接总部基地、商务中心和绿创大厦三个负荷。为了满足每个新增节点连接三个其他节点的规则，需要新增节点9，作为第二个分支点。\n",
            "\n",
            "4. **节点10:**  由于总部基地、商务中心和绿创大厦三个负荷位于同一方向，为了避免负荷节点之间直接相连，需要在节点9之后再新增一个节点10，连接这三个负荷。\n",
            "\n",
            "**拓扑结构图解 (文字版):**\n",
            "\n",
            "```\n",
            "热源 -- 节点8 -- 北郡 -- 光明府\n",
            "     |\n",
            "     -- 节点8 -- 节点9 -- 壹号天禧\n",
            "                   |\n",
            "                   -- 节点9 -- 节点10 -- 总部基地\n",
            "                                     |\n",
            "                                     -- 节点10 -- 商务中心\n",
            "                                     |\n",
            "                                     -- 节点10 -- 绿创大厦\n",
            "```\n",
            "\n",
            "\n",
            "这个拓扑结构满足所有规则：\n",
            "\n",
            "* 所有负荷都连接到热源。\n",
            "* 管道沿着道路铺设，没有穿过建筑物或湖泊。\n",
            "* 新增的节点8、9和10都连接到三个其他节点。\n",
            "* 热源是唯一的源节点，负荷是目标节点。\n",
            "* 负荷节点之间没有直接连接。\n",
            "\n",
            "\n",
            "这个方案力求简洁高效，并符合实际的供热管网布局原则。\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "经过重新审视，我之前的方案有一个错误：节点9只连接了两个节点（壹号天禧和节点10），违反了每个新增节点必须连接三个节点的规则。\n",
            "\n",
            "因此，我重新设计了方案，并仔细检查以确保符合所有规则。新的方案如下：\n",
            "\n",
            "```\n",
            "[\n",
            "(1, 8),  # 热源 -> 节点8\n",
            "(8, 5),  # 节点8 -> 北郡\n",
            "(8, 6),  # 节点8 -> 光明府  (修改：直接连接光明府)\n",
            "(1, 9),  # 热源 -> 节点9 (修改：从热源新建分支)\n",
            "(9, 7),  # 节点9 -> 壹号天禧\n",
            "(9, 10), # 节点9 -> 节点10\n",
            "(10, 2), # 节点10 -> 总部基地\n",
            "(10, 3), # 节点10 -> 商务中心\n",
            "(10, 4)  # 节点10 -> 绿创大厦\n",
            "]\n",
            "\n",
            "mark = 1 \n",
            "```\n",
            "\n",
            "**改进说明:**\n",
            "\n",
            "* **节点8 和 节点9 的连接方式修改:**  为了让节点8 和 节点9 都连接三个节点，将光明府直接连接到节点8，并从热源新建一个分支连接到节点9。\n",
            "\n",
            "**拓扑结构图解 (文字版):**\n",
            "\n",
            "```\n",
            "热源 -- 节点8 -- 北郡\n",
            "     |\n",
            "     -- 节点8 -- 光明府\n",
            "     |\n",
            "     -- 热源 -- 节点9 -- 壹号天禧\n",
            "                   |\n",
            "                   -- 节点9 -- 节点10 -- 总部基地\n",
            "                                     |\n",
            "                                     -- 节点10 -- 商务中心\n",
            "                                     |\n",
            "                                     -- 节点10 -- 绿创大厦\n",
            "```\n",
            "\n",
            "现在，这个方案完全符合所有规则：\n",
            "\n",
            "* 所有负荷都连接到热源。\n",
            "* 管道沿着道路铺设（假设地图上热源到光明府和壹号天禧都有道路）。\n",
            "* 新增的节点8、9和10都连接到三个其他节点。\n",
            "* 热源是唯一的源节点，负荷是目标节点。\n",
            "* 负荷节点之间没有直接连接。\n",
            "\n",
            "\n",
            "这个改进的方案更加合理，并且更符合供热管网设计的实际情况。因为它减少了不必要的节点和管道的长度，从而降低了成本和热量损失。\n",
            "\n",
            "2\n",
            "你说的对，我之前的方案仍然存在问题。虽然节点8和9都连接了三个节点，但是我忽略了题目的一个重要限制：**同方向的管道必须使用相同的主管道**。在我之前的方案中，总部基地、商务中心和绿创大厦与壹号天禧在同一条主管道上，但光明府却连接到了另一条主管道上，这违反了规则。\n",
            "\n",
            "为了解决这个问题，我再次重新设计了方案：\n",
            "\n",
            "```\n",
            "[\n",
            "(1, 8),  # 热源 -> 节点8\n",
            "(8, 2),  # 节点8 -> 总部基地\n",
            "(8, 9),  # 节点8 -> 节点9\n",
            "(9, 3),  # 节点9 -> 商务中心\n",
            "(9, 4),  # 节点9 -> 绿创大厦\n",
            "(1, 10), # 热源 -> 节点10\n",
            "(10, 7), # 节点10 -> 壹号天禧\n",
            "(10, 11),# 节点10 -> 节点11\n",
            "(11, 5), # 节点11 -> 北郡\n",
            "(11, 6)  # 节点11 -> 光明府\n",
            "]\n",
            "mark = 1\n",
            "```\n",
            "\n",
            "**改进说明:**\n",
            "\n",
            "* **重新划分主管道:**  现在有两条清晰的主管道，都直接从热源出发：\n",
            "    * **主管道1:** 热源 -> 总部基地 -> 商务中心 -> 绿创大厦\n",
            "    * **主管道2:** 热源 -> 壹号天禧 -> 北郡 -> 光明府\n",
            "\n",
            "* **新增节点11:** 为了满足规则3（新增节点连接三个其他节点），在连接北郡和光明府之前新增了节点11。\n",
            "\n",
            "**拓扑结构图解 (文字版):**\n",
            "\n",
            "```\n",
            "热源 -- 节点8 -- 总部基地\n",
            "     |\n",
            "     -- 节点8 -- 节点9 -- 商务中心\n",
            "                   |\n",
            "                   -- 节点9 -- 绿创大厦\n",
            "\n",
            "热源 -- 节点10 -- 壹号天禧\n",
            "     |\n",
            "     -- 节点10 -- 节点11 -- 北郡\n",
            "                       |\n",
            "                       -- 节点11 -- 光明府\n",
            "```\n",
            "\n",
            "现在这个方案：\n",
            "\n",
            "* 符合所有题目规则。\n",
            "* 更加清晰地体现了主干道的概念。\n",
            "* 尽可能地减少了管道长度和节点数量，提高了效率。\n",
            "\n",
            "\n",
            "我相信这个方案是更合理的，并且更贴近实际的工程设计。经过多次修改和思考，我更加理解了题目的要求和限制，并努力提供了一个更优的解决方案。\n",
            "\n",
            "3\n",
            "我的上一个方案确实还是有问题。问题在于我的思维仍然局限于必须有两条主管道。题目中“两条主管道”的描述更像是对地图现状的解读，而不是设计上的硬性要求。  如果我们不局限于两条主管道，可以设计出更简洁高效的方案。\n",
            "\n",
            "经过再次思考和改进，我提出了以下方案：\n",
            "\n",
            "```\n",
            "[\n",
            "(1, 8),  # 热源 -> 节点8\n",
            "(8, 2),  # 节点8 -> 总部基地\n",
            "(8, 3),  # 节点8 -> 商务中心\n",
            "(8, 4),  # 节点8 -> 绿创大厦\n",
            "(1, 9),  # 热源 -> 节点9\n",
            "(9, 7),  # 节点9 -> 壹号天禧\n",
            "(9, 5),  # 节点9 -> 北郡\n",
            "(9, 6)   # 节点9 -> 光明府\n",
            "]\n",
            "mark = 1\n",
            "```\n",
            "\n",
            "**改进说明:**\n",
            "\n",
            "* **不再局限于两条主管道:**  这次的设计思路是从热源出发，构建两个主要分支节点（节点8和节点9），分别连接邻近的负荷。\n",
            "\n",
            "* **简化了网络结构:**  这个方案减少了节点数量和管道长度，比之前的方案更加简洁高效。\n",
            "\n",
            "**拓扑结构图解 (文字版):**\n",
            "\n",
            "```\n",
            "热源 -- 节点8 -- 总部基地\n",
            "     |\n",
            "     -- 节点8 -- 商务中心\n",
            "     |\n",
            "     -- 节点8 -- 绿创大厦\n",
            "\n",
            "热源 -- 节点9 -- 壹号天禧\n",
            "     |\n",
            "     -- 节点9 -- 北郡\n",
            "     |\n",
            "     -- 节点9 -- 光明府\n",
            "```\n",
            "\n",
            "这个方案：\n",
            "\n",
            "* **完全符合所有规则：** 包括新增节点连接三个其他节点，负荷节点之间不相连，热源是唯一源节点等。\n",
            "* **更加简洁高效：** 减少了不必要的节点和管道，降低了成本和热量损失。\n",
            "* **更具灵活性：**  更容易根据实际情况进行调整和扩展。\n",
            "\n",
            "我认为这个方案是目前为止最优的解决方案，它在满足所有规则的前提下，最大程度地简化了网络结构，提高了效率。  经过反复的思考和改进，我终于找到了一个更合理、更实用的方案。\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1391.44ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TooManyRequests",
          "evalue": "429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-8f289ef5e261>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0mhistory_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    576\u001b[0m             )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         response = self.model.generate_content(\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    831\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3SbQujVrsbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "                        api_key=\"sk-8ef9eb1760894e4aac9686f2c34fc235\",\n",
        "                        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "                        )"
      ],
      "metadata": {
        "id": "_Ffv1pA6sjC-",
        "outputId": "4222fd78-4d4a-4a06-d810-4260ea86d352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OpenAI.__init__() got an unexpected keyword argument 'proxy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-0442316746bc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m client = OpenAI(\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sk-8ef9eb1760894e4aac9686f2c34fc235\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"httpx==0.28.0 #1902\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         )\n",
            "\u001b[0;31mTypeError\u001b[0m: OpenAI.__init__() got an unexpected keyword argument 'proxy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"sk-8ef9eb1760894e4aac9686f2c34fc235\",base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")"
      ],
      "metadata": {
        "id": "-U2SKV30rt4V",
        "outputId": "2951a295-f190-4742-a25e-cbae2a78cc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-d525db719cb2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sk-8ef9eb1760894e4aac9686f2c34fc235\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://api.openai.com/v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0m_strict_response_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_strict_response_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         )\n\u001b[0;32m--> 857\u001b[0;31m         self._client = http_client or SyncHttpxClientWrapper(\n\u001b[0m\u001b[1;32m    858\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0;31m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"limits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_CONNECTION_LIMITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"follow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_cOHoONrtBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bu_tr2ZAqVZ6",
        "outputId": "e02a4ee9-4184-48e5-a678-df6b06cfbc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'请你思考这个结果是否合理，是否遵循规则，如果遵守则输出mark=1，如果不遵守根据规则改进'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noELMecqa2hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kWIuwKG2_oWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08240b9b-8051-47c3-89a8-1717b5913959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Uploading: /gdrive/.shortcut-targets-by-id/1veAOqNEKzwAdn07ZfprTlEQpRJDizSmC/2.jpg\n",
            "[\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"file_data\": {\n",
            "                    \"mime_type\": \"application/octet-stream\",\n",
            "                    \"file_uri\": \"https://generativelanguage.googleapis.com/v1beta/files/n9dvci9kknis\"\n",
            "                }\n",
            "            },\n",
            "            {\n",
            "                \"text\": \"\\u8be5\\u56fe\\u662f\\u4e00\\u4e2a\\u9700\\u8981\\u8fdb\\u884c\\u533a\\u57df\\u4f9b\\u70ed\\u7cfb\\u7edf\\u89c4\\u5212\\u8bbe\\u8ba1\\u7684\\u5bf9\\u8c61\\uff0c\\u73b0\\u6709\\u70ed\\u6e90\\u548c\\u516d\\u4e2a\\u8d1f\\u8377\\uff0c\\u9700\\u8981\\u4f60\\u8fdb\\u884c\\u8d1f\\u8d23\\u8bbe\\u8ba1\\u4e00\\u4e2a\\u4f9b\\u70ed\\u7cfb\\u7edf\\u62d3\\u6251\\u7ed3\\u6784\\u3002\\u4f60\\u9700\\u8981\\u5bf9\\u5404\\u4e2a\\u5bf9\\u8c61\\u548c\\u7ba1\\u9053\\u4ea4\\u63a5\\u5904\\u8bbe\\u7f6e\\u8282\\u70b9\\uff0c\\u8282\\u70b9\\u8fde\\u63a5\\u4ee3\\u8868\\u7ba1\\u9053\\uff0c\\u8fd9\\u4e9b\\u7ba1\\u9053\\u4e0d\\u80fd\\u7a7f\\u8fc7\\u5efa\\u7b51\\u53ea\\u5728\\u56fe\\u4e2d\\u7684\\u9053\\u8def\\u8fdb\\u884c\\u8bbe\\u8ba1\\u3002\\\\n\\u5982\\u679c\\u540c\\u5411\\u6709\\u591a\\u4e2a\\u8d1f\\u8377\\uff0c\\u90a3\\u4e48\\u8be5\\u4e3b\\u7ba1\\u9053\\u9700\\u8981\\u8fdb\\u884c\\u5206\\u652f\\uff0c\\u5e76\\u4e14\\u5206\\u652f\\u540e\\u7684\\u65b0\\u8282\\u70b9\\u8fde\\u63a5\\u81f3\\u8fd9\\u4e24\\u4e2a\\u8d1f\\u8377\\u3002\\\\n\\u540c\\u4e00\\u4e2a\\u8282\\u70b9\\u4e0d\\u80fd\\u8fde\\u63a5\\u8d85\\u8fc73\\u4e2a\\u7ba1\\u9053\\uff0c\\u5982\\u679c\\u6709\\u9700\\u8981\\u8bbe\\u8ba1\\u7ba1\\u9053\\u5206\\u652f\\u3002\\\\n\\\\n\\u53c2\\u8003\\uff1a\\u70ed\\u6e90\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a1\\uff1b\\u603b\\u90e8\\u57fa\\u5730\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a2\\uff1b\\u5546\\u52a1\\u4e2d\\u5fc3\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a3\\uff1b\\u7eff\\u521b\\u5927\\u53a6\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a4\\uff1b\\u5317\\u90e1\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a5\\uff1b\\u5149\\u660e\\u5e9c\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a6\\uff1b\\u58f9\\u53f7\\u5929\\u79a7\\u8282\\u70b9\\u7f16\\u53f7\\u4e3a7\\u3002\\u4f60\\u8fd8\\u9700\\u8981\\u5bf9\\u5176\\u4ed6\\u8282\\u70b9\\u989d\\u5916\\u5b9a\\u4e49\\u7f16\\u53f7\\u3002\\\\n\\\\n\\u8bf7\\u4f60\\u7528\\u8fd4\\u56de\\u4e00\\u4e2a\\u5217\\u8868\\uff0c\\u5176\\u4e2d\\u5217\\u8868\\u5143\\u7d20\\u5305\\u542b\\uff08node_id1,node_id2\\uff09\\u4ee3\\u8868\\u4ece\\u8282\\u70b9\\u7f16\\u53f7node_id1\\u6d41\\u5165\\u8282\\u70b9\\u7f16\\u53f7node_id2\\u3002\\\\n\\u7ed3\\u679c\\u6a21\\u7248\\u4f8b\\u5982\\uff1a\\u201c[\\\\n(1, 5), # \\u70ed\\u6e90 -> \\u8282\\u70b95\\\\n(5, 6), # \\u8282\\u70b95 -> \\u8282\\u70b96\\\\n(6, 2), # \\u8282\\u70b96 -> \\u603b\\u90e8\\u57fa\\u5730\\\\n(6, 4), # \\u8282\\u70b96 -> \\u58f9\\u53f7\\u5929\\u79a7\\\\n(5, 3) # \\u8282\\u70b95 -> \\u5546\\u52a1\\u4e2d\\u5fc3\\\\n]\\u201d\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"model\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"```\\\\n[\\\\n(1, 8),  # \\u70ed\\u6e90 -> \\u8282\\u70b98\\\\n(8, 9),  # \\u8282\\u70b98 -> \\u8282\\u70b99\\\\n(9, 7),  # \\u8282\\u70b99 -> \\u58f9\\u53f7\\u5929\\u79a7\\\\n(9, 2),  # \\u8282\\u70b99 -> \\u603b\\u90e8\\u57fa\\u5730\\\\n(8, 10), # \\u8282\\u70b98 -> \\u8282\\u70b910\\\\n(10, 11),# \\u8282\\u70b910 -> \\u8282\\u70b911\\\\n(11, 4), # \\u8282\\u70b911 -> \\u7eff\\u521b\\u5927\\u53a6\\\\n(11, 3), # \\u8282\\u70b911 -> \\u5546\\u52a1\\u4e2d\\u5fc3\\\\n(10, 12),# \\u8282\\u70b910 -> \\u8282\\u70b912\\\\n(12, 5), # \\u8282\\u70b912 -> \\u5317\\u90e1\\\\n(12, 6), # \\u8282\\u70b912 -> \\u5149\\u660e\\u5e9c\\\\n]\\\\n```\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"🔑\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = 'gemini-1.5-pro' # @param {isTemplate: true}\n",
        "contents_b64 = 'W3sicm9sZSI6InVzZXIiLCJwYXJ0cyI6W3siZmlsZV9kYXRhIjp7Im1pbWVfdHlwZSI6ImFwcGxpY2F0aW9uL29jdGV0LXN0cmVhbSIsImRyaXZlX2lkIjoiMXZlQU9xTkVLendBZG4wN1pmcHJUbEVRcFJKRGl6U21DIn19LHsidGV4dCI6IuivpeWbvuaYr+S4gOS4qumcgOimgei/m+ihjOWMuuWfn+S+m+eDreezu+e7n+inhOWIkuiuvuiuoeeahOWvueixoe+8jOeOsOacieeDrea6kOWSjOWFreS4qui0n+iNt++8jOmcgOimgeS9oOi/m+ihjOi0n+i0o+iuvuiuoeS4gOS4quS+m+eDreezu+e7n+aLk+aJkee7k+aehOOAguS9oOmcgOimgeWvueWQhOS4quWvueixoeWSjOeuoemBk+S6pOaOpeWkhOiuvue9ruiKgueCue+8jOiKgueCuei/nuaOpeS7o+ihqOeuoemBk++8jOi/meS6m+euoemBk+S4jeiDveepv+i/h+W7uuetkeWPquWcqOWbvuS4reeahOmBk+i3r+i/m+ihjOiuvuiuoeOAglxcbuWmguaenOWQjOWQkeacieWkmuS4qui0n+iNt++8jOmCo+S5iOivpeS4u+euoemBk+mcgOimgei/m+ihjOWIhuaUr++8jOW5tuS4lOWIhuaUr+WQjueahOaWsOiKgueCuei/nuaOpeiHs+i/meS4pOS4qui0n+iNt+OAglxcbuWQjOS4gOS4quiKgueCueS4jeiDvei/nuaOpei2hei/hzPkuKrnrqHpgZPvvIzlpoLmnpzmnInpnIDopoHorr7orqHnrqHpgZPliIbmlK/jgIJcXG5cXG7lj4LogIPvvJrng63mupDoioLngrnnvJblj7fkuLox77yb5oC76YOo5Z+65Zyw6IqC54K557yW5Y+35Li6Mu+8m+WVhuWKoeS4reW/g+iKgueCuee8luWPt+S4ujPvvJvnu7/liJvlpKfljqboioLngrnnvJblj7fkuLo077yb5YyX6YOh6IqC54K557yW5Y+35Li6Ne+8m+WFieaYjuW6nOiKgueCuee8luWPt+S4ujbvvJvlo7nlj7flpKnnpqfoioLngrnnvJblj7fkuLo344CC5L2g6L+Y6ZyA6KaB5a+55YW25LuW6IqC54K56aKd5aSW5a6a5LmJ57yW5Y+344CCXFxuXFxu6K+35L2g55So6L+U5Zue5LiA5Liq5YiX6KGo77yM5YW25Lit5YiX6KGo5YWD57Sg5YyF5ZCr77yIbm9kZV9pZDEsbm9kZV9pZDLvvInku6Pooajku47oioLngrnnvJblj7dub2RlX2lkMea1geWFpeiKgueCuee8luWPt25vZGVfaWQy44CCXFxu57uT5p6c5qih54mI5L6L5aaC77ya4oCcW1xcbigxLCA1KSwgIyDng63mupAgLT4g6IqC54K5NVxcbig1LCA2KSwgIyDoioLngrk1IC0+IOiKgueCuTZcXG4oNiwgMiksICMg6IqC54K5NiAtPiDmgLvpg6jln7rlnLBcXG4oNiwgNCksICMg6IqC54K5NiAtPiDlo7nlj7flpKnnpqdcXG4oNSwgMykgIyDoioLngrk1IC0+IOWVhuWKoeS4reW/g1xcbl3igJ0ifV19LHsicm9sZSI6Im1vZGVsIiwicGFydHMiOlt7InRleHQiOiJgYGBcXG5bXFxuKDEsIDgpLCAgIyDng63mupAgLT4g6IqC54K5OFxcbig4LCA5KSwgICMg6IqC54K5OCAtPiDoioLngrk5XFxuKDksIDcpLCAgIyDoioLngrk5IC0+IOWjueWPt+Wkqeemp1xcbig5LCAyKSwgICMg6IqC54K5OSAtPiDmgLvpg6jln7rlnLBcXG4oOCwgMTApLCAjIOiKgueCuTggLT4g6IqC54K5MTBcXG4oMTAsIDExKSwjIOiKgueCuTEwIC0+IOiKgueCuTExXFxuKDExLCA0KSwgIyDoioLngrkxMSAtPiDnu7/liJvlpKfljqZcXG4oMTEsIDMpLCAjIOiKgueCuTExIC0+IOWVhuWKoeS4reW/g1xcbigxMCwgMTIpLCMg6IqC54K5MTAgLT4g6IqC54K5MTJcXG4oMTIsIDUpLCAjIOiKgueCuTEyIC0+IOWMl+mDoVxcbigxMiwgNiksICMg6IqC54K5MTIgLT4g5YWJ5piO5bqcXFxuXVxcbmBgYCJ9XX1d' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MCwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "\n",
        "        # if drive is None:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"📁 Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LB2LxPmAB95V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "76e620e4-adb9-4464-aa51-f665a2d628c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "400 Unable to submit request because it has a mimeType parameter with value application/octet-stream, which is not supported. Update the mimeType and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9d959b37a0ec>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgemini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m response = gemini.generate_content(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    831\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: 400 Unable to submit request because it has a mimeType parameter with value application/octet-stream, which is not supported. Update the mimeType and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini"
          ]
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9d345e9868"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Docs on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />More notebooks in the Cookbook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoL3p3KPylFW"
      },
      "outputs": [],
      "source": [
        "# @title Show the conversation, in colab.\n",
        "import mimetypes\n",
        "\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"📁 Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "\n",
        "\n",
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}